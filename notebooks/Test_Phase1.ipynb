{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª CogniSense Phase 1 Test Notebook\n",
    "\n",
    "This notebook tests all components of Phase 1 to ensure everything works correctly.\n",
    "\n",
    "**Expected Result**: All tests pass âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers datasets\n",
    "!pip install pillow numpy pandas matplotlib seaborn\n",
    "!pip install scikit-learn scipy\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "if not os.path.exists('AI4Alzheimers'):\n",
    "    !git clone https://github.com/Arnavsharma2/AI4Alzheimers.git\n",
    "    %cd AI4Alzheimers\n",
    "else:\n",
    "    %cd AI4Alzheimers\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test suite\n",
    "!python test_phase1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.synthetic_data_generator import (\n",
    "    EyeTrackingGenerator,\n",
    "    TypingDynamicsGenerator,\n",
    "    ClockDrawingGenerator,\n",
    "    GaitDataGenerator\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate samples\n",
    "eye_gen = EyeTrackingGenerator()\n",
    "typing_gen = TypingDynamicsGenerator()\n",
    "clock_gen = ClockDrawingGenerator()\n",
    "gait_gen = GaitDataGenerator()\n",
    "\n",
    "# Control samples\n",
    "eye_control = eye_gen.generate_sequence(is_alzheimers=False)\n",
    "typing_control = typing_gen.generate_sequence(is_alzheimers=False)\n",
    "clock_control = clock_gen.generate_image(is_alzheimers=False)\n",
    "gait_control = gait_gen.generate_sequence(is_alzheimers=False)\n",
    "\n",
    "# AD samples\n",
    "eye_ad = eye_gen.generate_sequence(is_alzheimers=True)\n",
    "typing_ad = typing_gen.generate_sequence(is_alzheimers=True)\n",
    "clock_ad = clock_gen.generate_image(is_alzheimers=True)\n",
    "gait_ad = gait_gen.generate_sequence(is_alzheimers=True)\n",
    "\n",
    "print(\"âœ… Sample data generated successfully!\")\n",
    "print(f\"Eye tracking: {eye_control.shape}\")\n",
    "print(f\"Typing: {typing_control.shape}\")\n",
    "print(f\"Clock: {clock_control.size}\")\n",
    "print(f\"Gait: {gait_control.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Eye tracking\n",
    "ax = axes[0, 0]\n",
    "ax.plot(eye_control[:, 0], eye_control[:, 1], 'b-', alpha=0.6, label='Control')\n",
    "ax.plot(eye_ad[:, 0], eye_ad[:, 1], 'r-', alpha=0.6, label='AD')\n",
    "ax.set_title('Eye Tracking Patterns')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Typing\n",
    "ax = axes[0, 1]\n",
    "ax.plot(typing_control[:, 0], 'b-', alpha=0.7, label='Control')\n",
    "ax.plot(typing_ad[:, 0], 'r-', alpha=0.7, label='AD')\n",
    "ax.set_title('Typing Dynamics (Flight Time)')\n",
    "ax.set_xlabel('Keystroke')\n",
    "ax.set_ylabel('Time (s)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Clock drawings\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(clock_control)\n",
    "ax.set_title('Clock Drawing - Control')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(clock_ad)\n",
    "ax.set_title('Clock Drawing - AD')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations rendered successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.fusion.fusion_model import MultimodalFusionModel\n",
    "from transformers import BertTokenizer, ViTImageProcessor\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing CogniSense model...\")\n",
    "model = MultimodalFusionModel(\n",
    "    speech_config={'freeze_encoders': True},\n",
    "    drawing_config={'freeze_encoder': True},\n",
    "    fusion_type='attention'\n",
    ")\n",
    "model.eval()\n",
    "print(f\"âœ… Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Prepare inputs\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Create inputs\n",
    "dummy_audio = torch.randn(1, 16000)\n",
    "dummy_text = \"The boy is taking the cookie\"\n",
    "\n",
    "speech_audio = {'input_values': dummy_audio}\n",
    "speech_text = bert_tokenizer(dummy_text, return_tensors='pt', padding=True, truncation=True)\n",
    "eye_tensor = torch.FloatTensor(eye_control).unsqueeze(0)\n",
    "typing_tensor = torch.FloatTensor(typing_control).unsqueeze(0)\n",
    "clock_processed = vit_processor(images=clock_control, return_tensors=\"pt\")\n",
    "drawing_tensor = clock_processed['pixel_values']\n",
    "gait_tensor = torch.FloatTensor(gait_control).unsqueeze(0)\n",
    "\n",
    "print(\"\\nRunning inference...\")\n",
    "with torch.no_grad():\n",
    "    risk_score, attention_weights, _ = model(\n",
    "        speech_audio=speech_audio,\n",
    "        speech_text=speech_text,\n",
    "        eye_gaze=eye_tensor,\n",
    "        typing_sequence=typing_tensor,\n",
    "        drawing_image=drawing_tensor,\n",
    "        gait_sensor=gait_tensor,\n",
    "        return_attention=True,\n",
    "        return_modality_features=True\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ… Inference successful!\")\n",
    "print(f\"   Risk Score: {risk_score.item()*100:.1f}%\")\n",
    "print(f\"   Attention Weights: {attention_weights[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Test: Compare AD vs Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(eye_data, typing_data, clock_img, gait_data):\n",
    "    \"\"\"Helper function to predict risk\"\"\"\n",
    "    dummy_audio = torch.randn(1, 16000)\n",
    "    dummy_text = \"The boy is taking the cookie\"\n",
    "    \n",
    "    speech_audio = {'input_values': dummy_audio}\n",
    "    speech_text = bert_tokenizer(dummy_text, return_tensors='pt', padding=True, truncation=True)\n",
    "    eye_tensor = torch.FloatTensor(eye_data).unsqueeze(0)\n",
    "    typing_tensor = torch.FloatTensor(typing_data).unsqueeze(0)\n",
    "    clock_processed = vit_processor(images=clock_img, return_tensors=\"pt\")\n",
    "    drawing_tensor = clock_processed['pixel_values']\n",
    "    gait_tensor = torch.FloatTensor(gait_data).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        risk_score, attention_weights, _ = model(\n",
    "            speech_audio=speech_audio,\n",
    "            speech_text=speech_text,\n",
    "            eye_gaze=eye_tensor,\n",
    "            typing_sequence=typing_tensor,\n",
    "            drawing_image=drawing_tensor,\n",
    "            gait_sensor=gait_tensor,\n",
    "            return_attention=True,\n",
    "            return_modality_features=True\n",
    "        )\n",
    "    \n",
    "    return risk_score.item(), attention_weights[0].cpu().numpy()\n",
    "\n",
    "# Test Control\n",
    "risk_control, att_control = predict_sample(eye_control, typing_control, clock_control, gait_control)\n",
    "print(f\"ðŸ”µ CONTROL Sample:\")\n",
    "print(f\"   Risk: {risk_control*100:.1f}%\")\n",
    "print(f\"   Attention: {att_control}\")\n",
    "\n",
    "# Test AD\n",
    "risk_ad, att_ad = predict_sample(eye_ad, typing_ad, clock_ad, gait_ad)\n",
    "print(f\"\\nðŸ”´ AD Sample:\")\n",
    "print(f\"   Risk: {risk_ad*100:.1f}%\")\n",
    "print(f\"   Attention: {att_ad}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Risk comparison\n",
    "ax = axes[0]\n",
    "ax.bar(['Control', 'AD'], [risk_control*100, risk_ad*100], color=['blue', 'red'], alpha=0.7)\n",
    "ax.set_ylabel('Risk Score (%)')\n",
    "ax.set_title('Risk Score Comparison')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Attention comparison\n",
    "ax = axes[1]\n",
    "modalities = ['Speech', 'Eye', 'Typing', 'Clock', 'Gait']\n",
    "x = np.arange(len(modalities))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, att_control*100, width, label='Control', color='blue', alpha=0.7)\n",
    "ax.bar(x + width/2, att_ad*100, width, label='AD', color='red', alpha=0.7)\n",
    "ax.set_ylabel('Attention Weight (%)')\n",
    "ax.set_title('Modality Importance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(modalities)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Comparison visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Phase 1 Test Summary\n",
    "\n",
    "If all cells above executed successfully, Phase 1 is **fully functional**!\n",
    "\n",
    "### What Works:\n",
    "- âœ… Synthetic data generation for all 5 modalities\n",
    "- âœ… All individual model architectures\n",
    "- âœ… Multimodal fusion with attention\n",
    "- âœ… Forward pass and inference\n",
    "- âœ… Visualizations and plotting\n",
    "- âœ… AD vs Control differentiation\n",
    "\n",
    "### Next Steps:\n",
    "1. Open `CogniSense_Demo.ipynb` for the full presentation notebook\n",
    "2. Proceed to Phase 2: Training Scripts\n",
    "3. Then Phase 3: Data Preprocessing\n",
    "4. Then Phase 4: Visualization Utilities\n",
    "5. Then Phase 5: Generate Results\n",
    "6. Finally Phase 6: Write PDF Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
