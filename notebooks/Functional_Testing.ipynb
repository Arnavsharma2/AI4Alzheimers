{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogniSense Functional Testing\n",
    "\n",
    "This notebook runs comprehensive functional tests for the CogniSense project.\n",
    "\n",
    "**Run this in Google Colab to verify all features work correctly.**\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count":  null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if running in Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    if not os.path.exists('AI4Alzheimers'):\n",
    "        !git clone https://github.com/Arnavsharma2/AI4Alzheimers.git\n",
    "    %cd AI4Alzheimers\n",
    "    !git checkout claude/review-drive-folder-01KHZ15iXzj7ZQnkH8rNKb62\n",
    "    print(\"âœ“ Repository cloned and checked out\")\n",
    "else:\n",
    "    print(\"âœ“ Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Import All Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing imports...\\n\")\n",
    "\n",
    "# Core dependencies\n",
    "import torch\n",
    "print(f\"âœ“ PyTorch {torch.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"âœ“ Transformers {transformers.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "\n",
    "import sklearn\n",
    "print(f\"âœ“ scikit-learn {sklearn.__version__}\")\n",
    "\n",
    "# CogniSense modules\n",
    "from src.models.eye_model import EyeTrackingModel\n",
    "from src.models.typing_model import TypingModel\n",
    "from src.models.drawing_model import ClockDrawingModel\n",
    "from src.models.gait_model import GaitModel\n",
    "from src.fusion.fusion_model import MultimodalFusionModel\n",
    "print(\"\\nâœ“ All CogniSense models imported\")\n",
    "\n",
    "from src.data_processing.synthetic_data_generator import (\n",
    "    EyeTrackingGenerator,\n",
    "    TypingDynamicsGenerator,\n",
    "    ClockDrawingGenerator,\n",
    "    GaitDataGenerator,\n",
    "    generate_synthetic_dataset\n",
    ")\n",
    "print(\"âœ“ All data generators imported\")\n",
    "\n",
    "from src.utils.training_utils import compute_metrics, EarlyStopping, train_epoch, evaluate\n",
    "from src.utils.visualization import plot_roc_curve, plot_confusion_matrix\n",
    "print(\"âœ“ All utilities imported\")\n",
    "\n",
    "print(\"\\nâœ… ALL IMPORTS SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating synthetic data...\\n\")\n",
    "\n",
    "# Eye tracking data\n",
    "eye_gen = EyeTrackingGenerator()\n",
    "eye_control = eye_gen.generate_sequence(is_alzheimers=False)\n",
    "eye_ad = eye_gen.generate_sequence(is_alzheimers=True)\n",
    "print(f\"âœ“ Eye tracking data: Control {eye_control.shape}, AD {eye_ad.shape}\")\n",
    "\n",
    "# Typing data\n",
    "typing_gen = TypingDynamicsGenerator()\n",
    "typing_control = typing_gen.generate_sequence(is_alzheimers=False)\n",
    "typing_ad = typing_gen.generate_sequence(is_alzheimers=True)\n",
    "print(f\"âœ“ Typing data: Control {typing_control.shape}, AD {typing_ad.shape}\")\n",
    "\n",
    "# Clock drawing\n",
    "drawing_gen = ClockDrawingGenerator()\n",
    "drawing_control = drawing_gen.generate_image(is_alzheimers=False)\n",
    "drawing_ad = drawing_gen.generate_image(is_alzheimers=True)\n",
    "print(f\"âœ“ Clock drawing: Control {drawing_control.shape}, AD {drawing_ad.shape}\")\n",
    "\n",
    "# Gait data\n",
    "gait_gen = GaitDataGenerator()\n",
    "gait_control = gait_gen.generate_sequence(is_alzheimers=False)\n",
    "gait_ad = gait_gen.generate_sequence(is_alzheimers=True)\n",
    "print(f\"âœ“ Gait data: Control {gait_control.shape}, AD {gait_ad.shape}\")\n",
    "\n",
    "print(\"\\nâœ… SYNTHETIC DATA GENERATION SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating models...\\n\")\n",
    "\n",
    "# Individual models\n",
    "eye_model = EyeTrackingModel()\n",
    "print(f\"âœ“ Eye model: {sum(p.numel() for p in eye_model.parameters())} parameters\")\n",
    "\n",
    "typing_model = TypingModel()\n",
    "print(f\"âœ“ Typing model: {sum(p.numel() for p in typing_model.parameters())} parameters\")\n",
    "\n",
    "print(\"  Loading ClockDrawingModel (downloading pretrained weights...)\")\n",
    "drawing_model = ClockDrawingModel()\n",
    "print(f\"âœ“ Drawing model: {sum(p.numel() for p in drawing_model.parameters())} parameters\")\n",
    "\n",
    "gait_model = GaitModel()\n",
    "print(f\"âœ“ Gait model: {sum(p.numel() for p in gait_model.parameters())} parameters\")\n",
    "\n",
    "# Fusion model\n",
    "fusion_model = MultimodalFusionModel(fusion_type='attention')\n",
    "print(f\"âœ“ Fusion model: {sum(p.numel() for p in fusion_model.parameters())} parameters\")\n",
    "\n",
    "print(\"\\nâœ… ALL MODELS INSTANTIATED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Forward Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing forward passes...\\n\")\n",
    "\n",
    "# Set models to eval mode\n",
    "eye_model.eval()\n",
    "typing_model.eval()\n",
    "drawing_model.eval()\n",
    "gait_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Eye model\n",
    "    eye_input = torch.FloatTensor(eye_ad).unsqueeze(0)\n",
    "    eye_output = eye_model(eye_input)\n",
    "    print(f\"âœ“ Eye model: {eye_input.shape} â†’ {eye_output.shape}\")\n",
    "    assert eye_output.shape == (1, 64), f\"Expected (1, 64), got {eye_output.shape}\"\n",
    "    \n",
    "    # Typing model\n",
    "    typing_input = torch.FloatTensor(typing_ad).unsqueeze(0)\n",
    "    typing_output = typing_model(typing_input)\n",
    "    print(f\"âœ“ Typing model: {typing_input.shape} â†’ {typing_output.shape}\")\n",
    "    assert typing_output.shape == (1, 64), f\"Expected (1, 64), got {typing_output.shape}\"\n",
    "    \n",
    "    # Drawing model\n",
    "    drawing_input = torch.FloatTensor(drawing_ad).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    drawing_output = drawing_model(drawing_input)\n",
    "    print(f\"âœ“ Drawing model: {drawing_input.shape} â†’ {drawing_output.shape}\")\n",
    "    assert drawing_output.shape == (1, 64), f\"Expected (1, 64), got {drawing_output.shape}\"\n",
    "    \n",
    "    # Gait model\n",
    "    gait_input = torch.FloatTensor(gait_ad).unsqueeze(0).permute(0, 2, 1)\n",
    "    gait_output = gait_model(gait_input)\n",
    "    print(f\"âœ“ Gait model: {gait_input.shape} â†’ {gait_output.shape}\")\n",
    "    assert gait_output.shape == (1, 64), f\"Expected (1, 64), got {gait_output.shape}\"\n",
    "\n",
    "print(\"\\nâœ… ALL FORWARD PASSES SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Multimodal Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing multimodal fusion...\\n\")\n",
    "\n",
    "fusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Prepare inputs\n",
    "    eye_in = torch.FloatTensor(eye_ad).unsqueeze(0)\n",
    "    typing_in = torch.FloatTensor(typing_ad).unsqueeze(0)\n",
    "    drawing_in = torch.FloatTensor(drawing_ad).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    gait_in = torch.FloatTensor(gait_ad).unsqueeze(0).permute(0, 2, 1)\n",
    "    \n",
    "    # Forward pass with attention\n",
    "    output, attention = fusion_model(\n",
    "        speech_audio=None,\n",
    "        speech_text=None,\n",
    "        eye_tracking=eye_in,\n",
    "        typing_dynamics=typing_in,\n",
    "        clock_drawing=drawing_in,\n",
    "        gait_data=gait_in,\n",
    "        return_attention=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Fusion output shape: {output.shape}\")\n",
    "    print(f\"âœ“ Attention weights shape: {attention.shape}\")\n",
    "    \n",
    "    # Verify attention sums to 1\n",
    "    attention_sum = attention.sum().item()\n",
    "    print(f\"âœ“ Attention sum: {attention_sum:.6f} (should be ~1.0)\")\n",
    "    assert abs(attention_sum - 1.0) < 1e-5, f\"Attention doesn't sum to 1: {attention_sum}\"\n",
    "    \n",
    "    # Calculate risk score\n",
    "    risk_score = torch.sigmoid(output).item()\n",
    "    print(f\"âœ“ AD risk score: {risk_score:.4f} (0-1, higher = more likely AD)\")\n",
    "    \n",
    "    # Display attention weights\n",
    "    print(\"\\nAttention weights:\")\n",
    "    modalities = ['Speech', 'Eye', 'Typing', 'Drawing', 'Gait']\n",
    "    for i, (mod, weight) in enumerate(zip(modalities, attention[0])):\n",
    "        bar = 'â–ˆ' * int(weight.item() * 50)\n",
    "        print(f\"  {mod:10s}: {weight.item():.4f} {bar}\")\n",
    "\n",
    "print(\"\\nâœ… MULTIMODAL FUSION SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dataset...\\n\")\n",
    "\n",
    "from src.data_processing.dataset import MultimodalAlzheimerDataset, custom_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Generate small dataset\n",
    "data_dict = generate_synthetic_dataset(\n",
    "    num_samples=20,\n",
    "    modalities=['eye', 'typing', 'drawing', 'gait']\n",
    ")\n",
    "print(f\"âœ“ Generated {len(data_dict['labels'])} samples\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = MultimodalAlzheimerDataset(\n",
    "    data_dict,\n",
    "    modalities=['eye', 'typing', 'drawing', 'gait']\n",
    ")\n",
    "print(f\"âœ“ Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Test single sample\n",
    "sample = dataset[0]\n",
    "print(f\"âœ“ Sample keys: {list(sample.keys())}\")\n",
    "print(f\"  Label: {sample['label']}\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "print(f\"âœ“ DataLoader created with batch_size=4\")\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"âœ“ Batch loaded:\")\n",
    "print(f\"  Eye: {len(batch['eye'])} samples\")\n",
    "print(f\"  Typing: {len(batch['typing'])} samples\")\n",
    "print(f\"  Drawing: {batch['drawing'].shape}\")\n",
    "print(f\"  Gait: {len(batch['gait'])} samples\")\n",
    "print(f\"  Labels: {batch['label'].shape}\")\n",
    "\n",
    "print(\"\\nâœ… DATASET CREATION SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing training utilities...\\n\")\n",
    "\n",
    "# Test metrics computation\n",
    "y_true = np.array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n",
    "y_pred = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1])\n",
    "y_prob = np.array([0.1, 0.2, 0.8, 0.9, 0.3, 0.7, 0.4, 0.2, 0.85, 0.6])\n",
    "\n",
    "metrics = compute_metrics(y_true, y_pred, y_prob)\n",
    "print(\"âœ“ Metrics computed:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key:15s}: {value:.4f}\")\n",
    "\n",
    "# Test early stopping\n",
    "early_stop = EarlyStopping(patience=3, mode='max')\n",
    "scores = [0.70, 0.75, 0.80, 0.85, 0.84, 0.83, 0.82, 0.81]\n",
    "stopped_at = None\n",
    "\n",
    "for epoch, score in enumerate(scores):\n",
    "    if early_stop(score):\n",
    "        stopped_at = epoch\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ“ Early stopping triggered at epoch {stopped_at}\")\n",
    "print(f\"  Best score: {early_stop.best_score:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… TRAINING UTILITIES SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Quick Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running quick training test (5 epochs)...\\n\")\n",
    "\n",
    "# Create small training dataset\n",
    "train_data = generate_synthetic_dataset(num_samples=50, modalities=['eye', 'typing', 'gait'])\n",
    "train_dataset = MultimodalAlzheimerDataset(train_data, modalities=['eye', 'typing', 'gait'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Create simple model\n",
    "test_model = EyeTrackingModel()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(test_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "test_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    test_model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Get eye tracking data only\n",
    "        inputs = [torch.FloatTensor(x).to(device) for x in batch['eye']]\n",
    "        labels = batch['label'].float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward passes for each sample (variable length)\n",
    "        outputs = []\n",
    "        for inp in inputs:\n",
    "            out = test_model(inp.unsqueeze(0))\n",
    "            outputs.append(out)\n",
    "        \n",
    "        # For simplicity, just use first output dimension\n",
    "        outputs = torch.cat([o[:, 0] for o in outputs])\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(labels)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/5: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… QUICK TRAINING TEST SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing visualization functions...\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.visualization import plot_confusion_matrix, plot_attention_heatmap\n",
    "\n",
    "# Generate fake predictions\n",
    "np.random.seed(42)\n",
    "y_true = np.random.randint(0, 2, 100)\n",
    "y_pred = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Confusion matrix\n",
    "fig = plot_confusion_matrix(y_true, y_pred, normalize=True)\n",
    "plt.show()\n",
    "print(\"âœ“ Confusion matrix plotted\")\n",
    "\n",
    "# Attention heatmap\n",
    "attention_weights = np.random.rand(10, 5)\n",
    "attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n",
    "modality_names = ['Speech', 'Eye', 'Typing', 'Drawing', 'Gait']\n",
    "\n",
    "fig = plot_attention_heatmap(attention_weights, modality_names)\n",
    "plt.show()\n",
    "print(\"âœ“ Attention heatmap plotted\")\n",
    "\n",
    "print(\"\\nâœ… VISUALIZATION SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  ðŸŽ‰ ALL FUNCTIONAL TESTS PASSED! ðŸŽ‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"  âœ… Module imports\")\n",
    "print(\"  âœ… Synthetic data generation\")\n",
    "print(\"  âœ… Model instantiation\")\n",
    "print(\"  âœ… Forward passes\")\n",
    "print(\"  âœ… Multimodal fusion\")\n",
    "print(\"  âœ… Dataset creation\")\n",
    "print(\"  âœ… Training utilities\")\n",
    "print(\"  âœ… Quick training\")\n",
    "print(\"  âœ… Visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  CogniSense is fully functional! ðŸš€\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
